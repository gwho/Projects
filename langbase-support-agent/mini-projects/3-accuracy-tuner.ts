/**
 * MINI-PROJECT 3: THE "ACCURACY TUNER"
 *
 * GOAL: Understand the precision vs. noise trade-off in retrieval
 *
 * LEARNING OBJECTIVES:
 * - See how top_k affects answer quality
 * - Understand when more context helps vs. hurts
 * - Learn to tune retrieval for different use cases
 *
 * THE TOP_K DILEMMA:
 * - Too low (k=1): Might miss important context ‚Üí incomplete answers
 * - Too high (k=20): Too much noise ‚Üí confused or verbose answers
 * - Sweet spot: Usually 3-5, but depends on your data!
 *
 * WHY THIS MATTERS:
 * - top_k directly affects answer quality AND cost
 * - More chunks = more tokens = higher API costs
 * - Finding the right balance is crucial for production
 *
 * TO RUN:
 *   tsx mini-projects/3-accuracy-tuner.ts
 */

import { Langbase } from 'langbase';
import * as dotenv from 'dotenv';

dotenv.config();

/**
 * Test different top_k values and compare results
 */
async function testTopKValues() {
  const apiKey = process.env.LANGBASE_API_KEY;
  if (!apiKey) {
    throw new Error('LANGBASE_API_KEY not found');
  }

  const langbase = new Langbase({ apiKey });
  const memoryName = process.env.MEMORY_NAME || 'support-faq-memory';

  console.log('\nüéØ ACCURACY TUNER EXPERIMENT\n');
  console.log('‚ïê'.repeat(70));
  console.log('Testing how top_k (number of retrieved chunks) affects answers\n');

  // Test with different questions
  const testCases = [
    {
      question: 'How do I upgrade my plan?',
      type: 'Specific question',
      expectedBehavior: 'Should work well with low top_k (answer is in one chunk)',
    },
    {
      question: 'Tell me about security and privacy.',
      type: 'Broad question',
      expectedBehavior: 'Needs higher top_k (info spread across multiple chunks)',
    },
    {
      question: 'What are the pricing options and can I get a refund?',
      type: 'Multi-part question',
      expectedBehavior: 'Needs moderate top_k (covers 2 topics)',
    },
  ];

  // Different top_k values to test
  const topKValues = [1, 3, 5, 10, 20];

  for (const testCase of testCases) {
    console.log('\n' + '‚ïê'.repeat(70));
    console.log(`üìù TEST CASE: ${testCase.type}`);
    console.log('‚ïê'.repeat(70));
    console.log(`\nQuestion: "${testCase.question}"`);
    console.log(`Expected: ${testCase.expectedBehavior}\n`);

    for (const topK of topKValues) {
      console.log('‚îÄ'.repeat(70));
      console.log(`üîç Testing with top_k = ${topK}`);
      console.log('‚îÄ'.repeat(70));

      try {
        // STEP 1: Retrieve chunks with this top_k
        const retrieval = await langbase.memory.retrieve({
          memoryName: memoryName,
          query: testCase.question,
          topK: topK,
        });

        console.log(`\nüìä Retrieved ${retrieval.data.length} chunks:\n`);

        // Show chunk quality distribution
        const scores = retrieval.data
          .map((chunk: any) => chunk.score)
          .filter((score: number) => score !== undefined);

        if (scores.length > 0) {
          const avgScore = scores.reduce((a: number, b: number) => a + b, 0) / scores.length;
          const highQuality = scores.filter((s: number) => s > 0.7).length;
          const mediumQuality = scores.filter((s: number) => s > 0.5 && s <= 0.7).length;
          const lowQuality = scores.filter((s: number) => s <= 0.5).length;

          console.log(`   Quality Distribution:`);
          console.log(`   ‚Ä¢ üü¢ High relevance (>70%): ${highQuality} chunks`);
          console.log(`   ‚Ä¢ üü° Medium relevance (50-70%): ${mediumQuality} chunks`);
          console.log(`   ‚Ä¢ üî¥ Low relevance (<50%): ${lowQuality} chunks`);
          console.log(`   ‚Ä¢ üìà Average similarity: ${(avgScore * 100).toFixed(1)}%\n`);

          // Calculate total tokens (rough estimate)
          const totalChars = retrieval.data.reduce(
            (sum: number, chunk: any) => sum + chunk.content.length,
            0
          );
          const estimatedTokens = Math.ceil(totalChars / 4); // Rough estimate: 1 token ‚âà 4 chars

          console.log(`   üìè Context Size:`);
          console.log(`   ‚Ä¢ Total characters: ${totalChars}`);
          console.log(`   ‚Ä¢ Estimated tokens: ~${estimatedTokens}`);
          console.log(`   ‚Ä¢ Cost impact: ${topK}x chunks = ${topK}x retrieval cost\n`);

          // Analysis
          console.log(`   üî¨ Analysis:`);
          if (topK === 1) {
            console.log(`   ‚Ä¢ Minimal context - might miss related info`);
            console.log(`   ‚Ä¢ Lowest cost`);
            console.log(`   ‚Ä¢ Best for: Very specific, single-fact questions`);
          } else if (topK <= 5) {
            console.log(`   ‚Ä¢ Balanced context - good for most questions`);
            console.log(`   ‚Ä¢ Moderate cost`);
            console.log(`   ‚Ä¢ Best for: Standard Q&A`);
          } else if (topK <= 10) {
            console.log(`   ‚Ä¢ Rich context - good for complex questions`);
            console.log(`   ‚Ä¢ Higher cost`);
            console.log(`   ‚Ä¢ Best for: Multi-part or broad questions`);
          } else {
            console.log(`   ‚Ä¢ Very rich context - potential noise`);
            console.log(`   ‚Ä¢ Highest cost`);
            console.log(`   ‚Ä¢ Best for: Research/exploration queries`);
            if (lowQuality > highQuality) {
              console.log(`   ‚ö†Ô∏è  WARNING: More low-quality than high-quality chunks!`);
              console.log(`   ‚ö†Ô∏è  This might confuse the LLM or add unnecessary cost`);
            }
          }
        }

        console.log('');

      } catch (error: any) {
        console.error(`   ‚ùå Error: ${error.message}\n`);
      }

      // Brief pause between requests
      await new Promise(resolve => setTimeout(resolve, 500));
    }

    console.log('\n');
  }

  // Summary and recommendations
  console.log('\n' + '‚ïê'.repeat(70));
  console.log('üìä SUMMARY & RECOMMENDATIONS\n');
  console.log('‚ïê'.repeat(70));

  console.log(`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  top_k  ‚îÇ     Use Case         ‚îÇ         Characteristics         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   1-2   ‚îÇ Specific facts       ‚îÇ ‚Ä¢ Fastest, cheapest             ‚îÇ
‚îÇ         ‚îÇ Simple Q&A           ‚îÇ ‚Ä¢ Risk: Missing context         ‚îÇ
‚îÇ         ‚îÇ                      ‚îÇ ‚Ä¢ Good for: "What is X?"        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   3-5   ‚îÇ Standard questions   ‚îÇ ‚Ä¢ RECOMMENDED for most cases    ‚îÇ
‚îÇ         ‚îÇ Balanced coverage    ‚îÇ ‚Ä¢ Good precision/recall balance ‚îÇ
‚îÇ         ‚îÇ                      ‚îÇ ‚Ä¢ Good for: "How do I...?"      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   5-10  ‚îÇ Complex queries      ‚îÇ ‚Ä¢ More comprehensive            ‚îÇ
‚îÇ         ‚îÇ Multi-part questions ‚îÇ ‚Ä¢ Higher cost                   ‚îÇ
‚îÇ         ‚îÇ                      ‚îÇ ‚Ä¢ Good for: "Explain..." + "..."‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  10-20  ‚îÇ Research mode        ‚îÇ ‚Ä¢ Maximum context               ‚îÇ
‚îÇ         ‚îÇ Exploratory queries  ‚îÇ ‚Ä¢ Highest cost, slowest         ‚îÇ
‚îÇ         ‚îÇ                      ‚îÇ ‚Ä¢ Risk: Information overload    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
`);

  console.log('\nüí° KEY INSIGHTS:\n');
  console.log('1. PRECISION vs. RECALL Trade-off:');
  console.log('   ‚Ä¢ Low top_k: High precision (only most relevant) but might miss info');
  console.log('   ‚Ä¢ High top_k: High recall (catch everything) but includes noise\n');

  console.log('2. COST Implications:');
  console.log('   ‚Ä¢ Every chunk costs tokens (both retrieval and LLM processing)');
  console.log('   ‚Ä¢ top_k=20 can be 4-5x more expensive than top_k=4');
  console.log('   ‚Ä¢ For production, optimize for minimum effective top_k\n');

  console.log('3. ANSWER QUALITY:');
  console.log('   ‚Ä¢ More chunks ‚â† better answers');
  console.log('   ‚Ä¢ Too much context can confuse the LLM');
  console.log('   ‚Ä¢ Watch for diminishing returns (scores drop significantly)\n');

  console.log('4. ADAPTIVE STRATEGIES:');
  console.log('   ‚Ä¢ Use similarity score thresholds (e.g., only use chunks >0.7)');
  console.log('   ‚Ä¢ Adjust top_k based on query complexity');
  console.log('   ‚Ä¢ Implement query classification (simple vs. complex)\n');

  console.log('\nüß™ EXPERIMENTS TO TRY:\n');
  console.log('1. Compare actual LLM answers (not just retrieval) for different top_k');
  console.log('2. Measure latency: How does top_k affect response time?');
  console.log('3. Cost analysis: Calculate actual token usage for different scenarios');
  console.log('4. Implement dynamic top_k based on query complexity');
  console.log('5. Add a similarity threshold filter (e.g., only chunks with score > 0.6)\n');

  console.log('üéØ NEXT STEPS:\n');
  console.log('1. Test YOUR specific use case with different top_k values');
  console.log('2. Monitor answer quality in production');
  console.log('3. A/B test different values with real users');
  console.log('4. Consider implementing adaptive retrieval strategies\n');
}

// Run the experiment
testTopKValues().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});
